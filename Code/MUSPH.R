#install.packages('scales')
# libraries used
#library(scales)
library(ROSE)
library(xgboost)
library(reshape)
library(janitor)
library(visdat)
library(kernlab)
library(caret) #ML Model buidling package
library(tidyverse) #ggplot and dplyr
library(MASS) #Modern Applied Statistics with S
library(mlbench) #data sets from the UCI repository.
library(summarytools)
library(corrplot) #Correlation plot
library(gridExtra) #Multiple plot in single grip space
library(timeDate) 
library(pROC) #ROC
library(caTools) #AUC
library(rpart.plot) #CART Decision Tree
library(e1071) #imports graphics, grDevices, class, stats, methods, utils
library(graphics) #fourfoldplot
library(naniar)# for missing values
library(mice)# for Imputation using mice
library(dplyr)# For Glimpse
#library(DMwR) it has failed to load for SMOTE 

# Code for loading the dataset
data <- read.csv("Dataset/trial.csv")
sapply(data, class)
head(data)

df <- data[sample(nrow(data)), ]
head(df)
# display 
print(df) 


# Code for checking for null value
sum(is.na(df))

#Plot of missing values
vis_miss(df) 
vis_miss(df, facet = Diabetes.Mellitus) 

library(ggplot2)
gg_miss_var(df, show_pct = TRUE) + ylim(0, 50)+ 
  labs(y ="All Missing values in Percentages")

#cleaning the dataset, 
df1 <- clean_names(df)
df1
#encoding the target variable as a factor
df1$diabetes_mellitus = factor(df1$diabetes_mellitus, 
                               levels = c(0,1),
                               labels = c("Negative", "Positive"))

#encoding sex variable
df1$sex = factor(df1$sex, 
                 levels = c(0,1),
                 labels = c("Male", "Female"))

#encoding Physical activity variable
df1$physical_activity = factor(df1$physical_activity, 
                               levels = c(0,1),
                               labels = c("Not active", "Active"))

#encoding Smoking status variable
df1$smoking_status = factor(df1$smoking_status, 
                            levels = c(0,1),
                            labels = c("non smoker", "smoker"))

#encoding Alcohol status variable
df1$alcohol_status = factor(df1$alcohol_status, 
                            levels = c(0,1),
                            labels = c("Non user", "user"))

#encoding Hypertension variable
df1$hypertension = factor(df1$hypertension, 
                          levels = c(0,1),
                          labels = c("Not sick", "Sick"))

#Mice package for imputing the missing values
data_imp = mice(df1,
                m=5, 
                method =c('pmm','','pmm',
                          'pmm','pmm','pmm','pmm',
                          'pmm','','','','',''), maxit = 50)
head(data_imp)
plot(data_imp)

data_imp$imp$bmi # to compare the output
final_data = complete(data_imp, 3)# selection of Final dataset to be used
glimpse(final_data)
df3 <- final_data
is.na(df3)#checking for missing values if still do exit in the final dataset

#Let’s start with oversampling and balance the data. Developing a balanced dataset using oversampling

data_df3 <- ovun.sample(diabetes_mellitus ~ ., data = df3, 
                                  method = "both", p=0.5,
                                  N=1920, seed = 1)$data
df4 <- data_df3

#checking to see if my data is normalized, if normalized SD= 1 , mean = 0
sd(df4$bmi)

## Normalising the dataset so that the mean is Zero and SD = 1

process <- preProcess(as.data.frame(df4), method=c("range"))
norm_scale <- predict(process, as.data.frame(df4))
df5 <- norm_scale

#calculate mean of scaled variable
sd(df5$bmi)

#Data Exploration Process starts here
#Distribution of Diabetes Mellitus in Training Dataset

#To select only numerical features from our dataset to plot boxplot.
Dout <- subset(df5, 
               select = c(age,glucose,
                          systolic,weight,
                          diastolic,bmi,insulin))

# Code for boxplot
ggplot(data = melt(Dout), 
       aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=variable))

#Table of summary statistics generated by Different variables
table1::table1(~age + sex + glucose + bmi + systolic + weight + diastolic 
               + physical_activity + smoking_status + alcohol_status
               +  hypertension | diabetes_mellitus,
               data = df5, 
               na.rm = TRUE,
               digits = 1, 
               format.number = TRUE)

# Convert categorical variables
data_new <- sapply(df5, unclass)          
data_new 
M <- data_new

# Default Heatmap
corrplot(cor(M), method="shade", main = "Heat Map",sub = "sub", 
         type = "full", shade.col=NA, tl.col="black", tl.srt=45)


#Data preparation
cutoff <- caret::createDataPartition(df5$diabetes_mellitus, times = 1, p=0.80, list=FALSE)
# 30% of the data will be used for validation
testdf <- df5[-cutoff,]
# 70% of data will be used for model training
traindf <- df5[cutoff,]

#Identifying variable of importance
library(Boruta)
boruta_output <- Boruta(diabetes_mellitus~., data=traindf, doTrace=0)
rough_fix_mod <- TentativeRoughFix(boruta_output)
boruta_signif <- getSelectedAttributes(rough_fix_mod)
importances <- attStats(rough_fix_mod)
importances <- importances[importances$decision !="Rejected", c("meanImp", "decision")]
importances[order(-importances$meanImp), ]

attStats(boruta_output)# look at the attribute statistics

plot(boruta_output, las =2, cex.axis=0.7, xlab="",
     main= "Important variables used development of Prediction models")

# Code to train the SVM, I set the 10 fold cross validation 
set.seed(223)

control <- trainControl(method="cv",number=10,  summaryFunction = twoClassSummary, classProbs = TRUE)
metric <- "ROC"
model_svm <- caret::train(diabetes_mellitus ~., 
                           
                          data = traindf,
                          method = "svmRadial",
                          metric = metric,
                          tuneLength = 8,
                          
                          trControl =control,
                          preProcess = c("center","scale"))
model_svm


#Code Let’s plot the accuracy graph.
plot((model_svm),main="ROC for Support Vector Model during Training")

# final ROC value
model_svm$results["ROC"]

#Prediction of SVM model using the Test Data.
predict_svm <- predict(model_svm, newdata = testdf)

#Confusion Matrix 
cm_svm <-confusionMatrix(predict_svm, testdf$diabetes_mellitus, positive="Positive")
cm_svm

# Prediction Probabilities
pred_prob_svm <- predict(model_svm, testdf, type="prob")

#ROC value
roc_svm <- roc(testdf$diabetes_mellitus, pred_prob_svm$Positive)
roc_svm

# Plot the ROC curve with AUC value
#plot(roc_svm, print.auc = TRUE, auc.polygon = TRUE, 
     #legacy.axes = TRUE, main = "Support Vector Model ROC Curve with AUC Value")
plot(roc_svm, 
     colorize=TRUE,
     avg='horizontal',
     spread.estimate='boxplot',
     lwd=3,
     col='purple',
     print.auc = TRUE, auc.polygon = TRUE, 
     legacy.axes = TRUE, main = "Support Vector Model ROC Curve with AUC Value")


#Random Forest Model
set.seed(223)
control <- trainControl(method="cv",number=10,  summaryFunction = twoClassSummary, classProbs = TRUE)
metric <- "ROC"
model_rfm<- caret::train(diabetes_mellitus ~., 
                
                        data = traindf,
                        method = "ranger",
                        metric = metric,
                        trControl = control,
                        preProcess = c("center","scale","pca"))
model_rfm

#Final ROC value
model_rfm$results["ROC"]
# Plotting model
plot(model_rfm, main="ROC for Random Forest Model during Training")


#Prediction/ validating the Random Forest model using the test dataset
predict_rfm <- predict(model_rfm, newdata = testdf)

#Confusion Matrix 
cm_rfm <-confusionMatrix(predict_rfm, testdf$diabetes_mellitus, positive="Positive")
cm_rfm

#Prediction Probabilities
pred_prob_rfm <- predict(model_rfm, testdf, type="prob")

#ROC value
roc_rfm <- roc(testdf$diabetes_mellitus, pred_prob_rfm$Positive)
roc_rfm

# Plot the ROC curve with AUC value
#plot(roc_rfm, print.auc = TRUE, auc.polygon = TRUE, 
     #legacy.axes = TRUE, main = "Random Forest ROC Curve with AUC Value")

plot(roc_rfm, 
     colorize=TRUE,
     avg='horizontal',
     spread.estimate='boxplot',
     lwd=3,
     col='pink',
     print.auc = TRUE, auc.polygon = TRUE, 
     legacy.axes = TRUE, main = "Random Forest ROC Curve with AUC Value")


# XGBOOST - eXtreme Gradient BOOSTing 
xgb_grid  <-  expand.grid(
  nrounds = 150,
  eta = c(0.03),
  max_depth = 1,
  gamma = 0,
  colsample_bytree = 0.6,
  min_child_weight = 1,
  subsample = 0.5
)
set.seed(223)
control <- trainControl(method="cv", number=10,  summaryFunction = twoClassSummary, classProbs = TRUE)
metric <- "ROC"
model_xgb <- caret::train(diabetes_mellitus ~., 
                          data = traindf,
                          method = "xgbTree",
                          metric = metric,
                          tuneGrid=xgb_grid,
                          trControl = control,
                          preProcess = c("center","scale","pca"))
model_xgb

#Final ROC value
model_xgb$results["ROC"]

# Plotting model
#plot(model_xgb)

#XGBOOST - eXtreme Gradient BOOSTing model using the test dataset
predict_xgb <- predict(model_xgb, newdata = testdf)

#Confusion Matrix 
cm_xgb <-confusionMatrix(predict_xgb, testdf$diabetes_mellitus, positive="Positive")
cm_xgb

#Prediction Probabilities
pred_prob_xgb <- predict(model_xgb, testdf, type="prob")

#ROC value
roc_xgb <- roc(testdf$diabetes_mellitus, pred_prob_xgb$Positive)
roc_xgb

# Plot the ROC curve with AUC value
#plot(roc_xgb, print.auc = TRUE, auc.polygon = TRUE, 
     #legacy.axes = TRUE, main = "Extreme Gradient Boosting ROC Curve with AUC Value")

plot(roc_xgb, 
     colorize=TRUE,
     avg='horizontal',
     spread.estimate='boxplot',
     lwd=3,
     col='black',
     print.auc = TRUE, auc.polygon = TRUE, 
     legacy.axes = TRUE, main = "Extreme Gradient Boosting ROC Curve with AUC Value")


##CODE FOR KNN, the tuneLength parameter tells the algorithm to try different default values for the main parameter
# The tuneGrid parameter lets us decide which values the main parameter will take
# While tuneLength only limit the number of default parameters to use.

set.seed(223)
control <- trainControl(method="cv", number=10,  summaryFunction = twoClassSummary, classProbs = TRUE)
metric <- "ROC"
model_knn <- caret::train(diabetes_mellitus ~., 
                          data = traindf,
                          method = "knn",
                          metric = metric,
                          tuneGrid = expand.grid(.k = c(5,7,9,11)),
                          trControl = control,
                          preProcess = c("center","scale","pca"))
model_knn
plot((model_knn), main="ROC for K_Nearest Neigbhour Model during Training")

# final ROC value
model_knn$results["ROC"]

#validating the K_Nearest Neigbhour model using the test dataset
predict_model_knn <- predict(model_knn, newdata = testdf)

# Confusion Matrix 
cm_knn <-confusionMatrix(predict_model_knn, testdf$diabetes_mellitus, positive="Positive")
cm_knn

# Prediction Probabilities
pred_prob_knn <- predict(model_knn, testdf, type="prob")

# ROC value
roc_knn <- roc(testdf$diabetes_mellitus, pred_prob_knn$Positive)
roc_knn

# Plot the ROC curve with AUC value
#plot(roc_knn, print.auc = TRUE, auc.polygon = TRUE, 
     #legacy.axes = TRUE, main = "K-Nearest Neighbor ROC Curve with AUC Value")

plot(roc_knn, 
     colorize=TRUE,
     avg='horizontal',
     spread.estimate='boxplot',
     lwd=3,
     col='orange',
     print.auc = TRUE, auc.polygon = TRUE, 
     legacy.axes = TRUE, main = "K-Nearest Neigbor ROC Curve with AUC Value")



# Decision Tree Model
set.seed(223)
control <- trainControl(method="cv", number=10,  summaryFunction = twoClassSummary, classProbs = TRUE)
metric <- "ROC"
model_dtm <- caret::train(diabetes_mellitus ~., 
                          data = traindf,
                          method = "rpart",
                          metric = metric,
                          tuneLength = 20,
                          trControl = control)
                          #preProcess = c("center","scale","pca"))

model_dtm

#Plot model accuracy vs different values of cp (complexity parameter)
plot((model_dtm), main="ROC for Decision Tree Model during Training")

#Final ROC value
model_dtm$results["ROC"] 

#Best Model Cp (Complexity Parameter)
model_dtm$bestTune

#Structure of final model selected
model_dtm$finalModel

#Plot of DTM 
rpart.plot::rpart.plot(model_dtm$finalModel, type =2, fallen.leaves = T, 
                       extra = 2, 
                       faclen=1,
                       under = TRUE,
                       roundint=F, #don't round to integers in output
                       digits=1, #display 5 decimal places in output
                       cex = 0.80,
                       box.palette = "auto")

#Validating the Decision Tree model using the test dataset
predict_dtm <- predict(model_dtm, newdata = testdf)

#Confusion Matrix 
cm_dtm <-confusionMatrix(predict_dtm, testdf$diabetes_mellitus, positive="Positive")
cm_dtm

#Prediction Probabilities
pred_prob_dtm <- predict(model_dtm, testdf, type="prob")

# ROC value
roc_dtm <- roc(testdf$diabetes_mellitus, pred_prob_dtm$Positive)
roc_dtm

# Plot the ROC curve with AUC value

plot(roc_dtm, 
     colorize=TRUE,
     avg='horizontal',
     spread.estimate='boxplot',
     lwd=3,
     col='steelblue',
     print.auc = TRUE, auc.polygon = TRUE, 
     legacy.axes = TRUE, main = "Decission Tree ROC Curve with AUC Value")


#Comparison of ROC values of different models using the Training Data set 
model_list <- list(Support_Vector = model_svm, 
                   Random_Forest = model_rfm,
                   XGBoost = model_xgb, 
                   KNN = model_knn, 
                   Rpart_DT = model_dtm)

resamples <- resamples(model_list)

#### Compare models and summarize/visualize results
if(length(model_list)> 1){
  resamp = resamples(model_list)
  # Summarize the results
  summary(resamp)
  dotplot(resamp, 
          ##main="Comparison of ROC values of models with the Training Data set", #
          metric="ROC")

}
xyplot(resamp, metric="ROC" )

#box plot
##bwplot(resamples, 
      # main="Comparison of ROC values of models with the Training Data set", 
       #metric="ROC")

#dot plot
#dotplot(resamples, metric="ROC")


# Comparision of test results using Test Set

result_rfm <- c(cm_rfm$byClass['Sensitivity'], cm_rfm$byClass['Specificity'], cm_rfm$byClass['Precision'], 
               cm_rfm$byClass['Recall'], cm_rfm$byClass['F1'], roc_rfm$auc)

result_xgb <- c(cm_xgb$byClass['Sensitivity'], cm_xgb$byClass['Specificity'], cm_xgb$byClass['Precision'], 
                cm_xgb$byClass['Recall'], cm_xgb$byClass['F1'], roc_xgb$auc)

result_knn <- c(cm_knn$byClass['Sensitivity'], cm_knn$byClass['Specificity'], cm_knn$byClass['Precision'], 
                cm_knn$byClass['Recall'], cm_knn$byClass['F1'], roc_knn$auc)

result_svm <- c(cm_svm$byClass['Sensitivity'], cm_svm$byClass['Specificity'], cm_svm$byClass['Precision'], 
                cm_svm$byClass['Recall'], cm_svm$byClass['F1'], roc_svm$auc)

result_dtm <- c(cm_dtm$byClass['Sensitivity'], cm_dtm$byClass['Specificity'], cm_dtm$byClass['Precision'], 
                  cm_dtm$byClass['Recall'], cm_dtm$byClass['F1'], roc_dtm$auc)


all_results <- data.frame(rbind(result_rfm, result_xgb, result_knn, result_svm, result_dtm))
names(all_results) <- c("Sensitivity", "Specificity", "Precision", "Recall", "F1", "AUC")
all_results


#Visualization to compare accuracy of models using Four Fold Plot of the developed Models 
col <- c("#91CBD765", "#CA225E")

graphics::fourfoldplot(cm_svm$table, color = col, conf.level = 0.95, margin = 1, 
                       main = paste("Support Vector Machine(",round(cm_svm$overall[1]*100),"%)", sep = ""))

graphics::fourfoldplot(cm_rfm$table, color = col, conf.level = 0.95, margin = 1, 
                       main = paste("Random Forest Accuracy(",round(cm_rfm$overall[1]*100),"%)", sep = ""))


graphics::fourfoldplot(cm_xgb$table, color = col, conf.level = 0.95, margin = 1, 
                       main = paste("Xtreme Gradient Boosting Accuracy(",round(cm_xgb$overall[1]*100),"%)", sep = ""))


graphics::fourfoldplot(cm_knn$table, color = col, conf.level = 0.95, margin = 1, 
                       main = paste("K-Nearest Neighbour Accuracy(",round(cm_knn$overall[1]*100),"%)", sep = ""))

graphics::fourfoldplot(cm_dtm$table, color = col, conf.level = 0.95, margin = 1, 
                       main = paste("Decission Tree Accuracy(",round(cm_dtm$overall[1]*100),"%)", sep = ""))
